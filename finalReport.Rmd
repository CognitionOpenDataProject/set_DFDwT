---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    float: true
---

#### Article ID: DFDwT
#### Pilot: Camilla Griffiths
#### Co-pilot: Gustav Nilsonne
#### Co-pilot 2: Bria Long
#### Start date: Mar 12 2017
#### End date: Oct 23 2017
#### Final verification: Tom Hardwicke
#### Date: Nov 9 2017

-------

#### Methods summary: 

After excluding trials based on stated exclusion criteria (i.e. extreme reproduction intervals: >3SD from individual & grand mean), the analyses will include a 2X2 ANOVA testing the main and interaction effects of 'presence of agency' and 'physical effort' on the accuracy of interval replication between two events. Participants in this within-subjects experiment were either asked to press a button and then listen for a tone (agency) or to listen to an initial tone then a second tone (no agency) while holding a resistance band in their right hand that was either high or low resistance/physical effort. 

I will also set contrasts to test the effects of each individual condition on the outcome of interval reproduction errors. These contrasts will entail comparing low vs. high physical effort and comparing agency vs. no agency conditions. 

------

#### Target outcomes: 


> "Trials with extreme reproduction errors were removed (0.48% of trials). Mean reproduction errors for each participant in each condition (see Fig. 3) were analysed using a 2 (“Presence of Agency”) × 2 (“Physical Effort”) ANOVA, revealing significant main effects of both of these factors: F(1, 34) = 54.54, p < 0.001, ηp2 = 0.62, and F(1, 34) = 14.43, p = 0.001, ηp2 = 0.3, respectively. These main effects were due to larger reproduction errors - more underestimation - in the “Agency” condition than the “No Agency” condition, (M = −322 ms, SD = 236 and M = −56 ms, SD = 265, respectively). This replicates the basic temporal binding effect ( Buehner and Humphreys, 2009; Haggard et al., 2002; Humphreys and Buehner, 2010 ; Poonian and Cunnington, 2013). There were larger reproduction errors in the “Low” effort compared with “High” effort conditions, M = -208 ms, SD = 230 and M = −170 ms, SD = 228, respectively. However, the critical interaction was non-significant, F(1, 34) = 2.12, p = 0.154, ηp2 = 0.06.

> Although the interaction – critical to our research question – was non-significant, we nevertheless performed two planned contrasts to investigate the nature of effects across “Presence of Agency” and “Physical Effort”. In the “Agency” condition, reproduction errors were significantly larger under “Low” than “High” effort, t(34) = 3.46, p = 0.001, dz = 0.589, while this effect was not significant in the “No Agency” condition, t(34) = 1.59, p = 0.122, dz = 0.27. Therefore, due to a non-significant interaction, the overall results do not support our hypothesis that reproduction errors in agency tasks would be reduced under greater physical effort." (from Howard et al, 2016 p.117)"

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)

# prepare an empty report object, we will update this each time we run compareValues2()
reportObject <- data.frame("Article_ID" = NA, "valuesChecked" = 0, "eyeballs" = 0, "Total_df" = 0, "Total_p" = 0, "Total_mean" = 0, "Total_sd" = 0, "Total_se" = 0, "Total_ci" = 0, "Total_bf" = 0, "Total_t" = 0, "Total_F" = 0, "Total_es" = 0, "Total_median" = 0, "Total_irr" = 0, "Total_r" = 0, "Total_z" = 0, "Total_coeff" = 0, "Total_n" = 0, "Total_x2" = 0, "Total_other" = 0, "Insufficient_Information_Errors" = 0, "Decision_Errors" = 0, "Major_Numerical_Errors" = 0, "Minor_Numerical_Errors" = 0, "Major_df" = 0, "Major_p" = 0, "Major_mean" = 0, "Major_sd" = 0, "Major_se" = 0, "Major_ci" = 0, "Major_bf" = 0, "Major_t" = 0, "Major_F" = 0, "Major_es" = 0, "Major_median" = 0, "Major_irr" = 0, "Major_r" = 0, "Major_z" = 0, "Major_coeff" = 0, "Major_n" = 0, "Major_x2" = 0, "Major_other" = 0, "affectsConclusion" = NA, "error_typo" = 0, "error_specification" = 0, "error_analysis" = 0, "error_data" = 0, "error_unidentified" = 0, "Author_Assistance" = NA, "resolved_typo" = 0, "resolved_specification" = 0, "resolved_analysis" = 0, "resolved_data" = 0, "correctionSuggested" = NA, "correctionPublished" = NA)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(ez) # for anova
```

## Step 2: Load data

I could not load the data as is into R because it was highly formatted with multiple tabs, headers and subheaders in Excel so I made the following changes in Excel in order to be able to import into R: 

1. Copied just Experiment 1 data (only data of interest here) into a new Excel workbook
2. Saved the new workbook as data_ManualClean
3. Changed 'Participant ID' to PID 
4. Changed condition variable labels so that it includes information about both conditions (to be able to get rid of double headers) 
  - Agency | low effort --> A_lowE
  - Agency | high effort --> A_highE
  - No Agency | low effort --> NA_lowE
  - No Agency | high effort --> NA_highE
5. Changed condition variable names to reflect order of trials -
  - Agency |1st half | low effort --> 1A_lowE
  - Agency |1st half | high effort --> 1A_highE
  - Agency |2nd half | low effort --> 2A_lowE
  - Agency |2nd half | high effort --> 2A_highE
  - No Agency |1st half | low effort --> 1NA_lowE
  - No Agency |1st half | high effort --> 1NA_highE
  - No Agency |2nd half | low effort --> 2NA_lowE
  - No Agency |2nd half | high effort --> 2NA_highE
6. Changed variable names to reflect pre- and post-task effort ratings -  again to be able to delete multiple headers and make variable names descriptive 
  - low effort pre --> preE_low
  - low effort post --> postE_low
  - high effort pre --> preE_high
  - high effort post --> preE_high
7. Changed main DV variable name from 'Reproduction Error Differential' to 'RepError_D' 
8. Changed 'Error Differential' variable name to 'Error_D'

```{r}
expt1 <- read_csv("data/data_ManualClean.csv", 
     col_types = cols(`Gender (1=female)` = col_factor(levels = c("Female", "Male"))))

expt1 <- expt1 %>%
  rowwise() %>%
  mutate(agency_effortCombined = mean(c(A_lowE, A_highE)),
         noagency_effortCombined = mean(c(NA_lowE, NA_highE)),
         loweffort_agencyCombined = mean(c(A_lowE, NA_lowE)),
         higheffort_agencyCombined = mean(c(A_highE, NA_highE)))
```

## Step 3: Tidy data

To reproduce analyses in the specified 2.2 results section, I will only be looking at the interval reproduction data in both conditions (effort & agency) - this means excluding the trial order variables as well as the manipulation check variables in this tidy dataframe. 

```{r}
expt1.tidy= expt1 %>%
  select(PID, A_lowE:NA_highE) %>%
  gather(title, rating, A_lowE:NA_highE) %>%
  separate(title, c("agency", "effort"), sep="_") %>%
  mutate(agency = recode(agency, `A` = "Agency", `NA` = "No Agency"))
```

## Step 4: Run analysis

### Pre-processing

As they did in their analysis, we'll remove trials that are >3SD from a participants' mean interval reproductions and to remove trials that are >3SD from the grand mean. 

```{r}
#filtering Ps trials that are >3SD from grand mean
expt1.tidy= expt1.tidy %>%
  mutate(zscore=scale(rating)) %>%
  filter(zscore<3)
  
#filtering Ps trials that are >3SD from their individual means
expt1.tidy= expt1.tidy %>%
  group_by(PID)%>%
  mutate(indiv_zscore=scale(rating)) %>%
  filter(indiv_zscore<3)

summary(expt1.tidy$zscore)
```

It looks like they might have done their exclusions prior to uploading their data - no trials appeared to be anywhere close to 3SD from the grand mean or individual means - the maximum zscore was 2.5, which is what leads me to believe they performed their exclusions before posting the data. 

Therefore, there were no trials excluded in this replication analysis. 

### Descriptive statistics

First attempt (without author assistance):

```{r}
# ## gather mean reproduction errors by agency condition
# agencyStats=expt1.tidy %>%
#   mutate(agency=as.factor(agency)) %>%
#   group_by(agency) %>%
#   summarise(mean_agency= mean(rating), SD_agency=sd(rating))
# 
# ## compare agency mean and sd (rounded) vs. reported values
# # agent = 1
# compareValues(obtainedValue = agencyStats$mean_agency[1], reportedValue = -322) # match
# compareValues(obtainedValue = agencyStats$SD_agency[1], reportedValue =  236) # minor error
# # non-agent = 2
# compareValues(obtainedValue = agencyStats$mean_agency[2], reportedValue = -56) # match
# compareValues(obtainedValue = agencyStats$SD_agency[2], reportedValue = 265) # minor error
# 
# ## gather reproduction errors by effort condition 
# effortStats=expt1.tidy %>%
#   mutate(effort=as.factor(effort)) %>%
#   group_by(effort) %>%
#   summarise(mean_effort= mean(rating), SD_effort=sd(rating))
# 
# ## compare effort mean and sd (rounded) vs. reported values
# # high effort
# compareValues(round(effortStats$mean_effort[1],0),-170) # match
# compareValues(round(effortStats$SD_effort[1],0),228) # major error
# # low effort
# compareValues(round(effortStats$mean_effort[2],0),-208) # match
# compareValues(round(effortStats$SD_effort[2],0),230) # major error
```

In our first attempt we encountered some errors. However, after discussing the issue with the authors we discovered that we were aggregating the data differently. 

Second attempt (with author assistance):

```{r}
## gather mean reproduction errors by agency condition
agencyEffortCombined_M = mean(expt1$agency_effortCombined)
agencyEffortCombined_SD = sd(expt1$agency_effortCombined)

## gather mean reproduction errors by agency condition
noagencyEffortCombined_M = mean(expt1$noagency_effortCombined)
noagencyEffortCombined_SD = sd(expt1$noagency_effortCombined)
```

> these main effects were due to larger reproduction errors - more underestimation - in the “Agency” condition than the “No Agency” condition, (M = −322 ms, SD = 236 and M = −56 ms, SD = 265, respectively).

```{r}
## compare agency mean and sd (rounded) vs. reported values
# agent = 1
reportObject <- compareValues2(obtainedValue = agencyEffortCombined_M, reportedValue = "-322", valueType = 'mean') # match
reportObject <- compareValues2(obtainedValue = agencyEffortCombined_SD, reportedValue =  "236", valueType = 'sd') # match

# non-agent = 2
reportObject <- compareValues2(obtainedValue = noagencyEffortCombined_M, reportedValue = "-56", valueType = 'mean') # match
reportObject <- compareValues2(obtainedValue = noagencyEffortCombined_SD, reportedValue = "265", valueType = 'sd') # match
```

```{r}
## gather reproduction errors by effort condition 
highEffortAgencyCombined_M = mean(expt1$higheffort_agencyCombined)
highEffortAgencyCombined_SD = sd(expt1$higheffort_agencyCombined)

## gather mean reproduction errors by agency condition
loweffortAgencyCombined_M = mean(expt1$loweffort_agencyCombined)
loweffortAgencyCombined_SD = sd(expt1$loweffort_agencyCombined)
```

> There were larger reproduction errors in the “Low” effort compared with “High” effort conditions, M = -208 ms, SD = 230 and M = −170 ms, SD = 228, respectively.

```{r}
## compare effort mean and sd (rounded) vs. reported values
# high effort
reportObject <- compareValues2(obtainedValue = highEffortAgencyCombined_M, reportedValue = "-170", valueType = 'mean') # match
reportObject <- compareValues2(obtainedValue = highEffortAgencyCombined_SD, reportedValue = "228", valueType = 'sd') # match

# low effort
reportObject <- compareValues2(obtainedValue = loweffortAgencyCombined_M,reportedValue = "-208", valueType = 'mean') # match
reportObject <- compareValues2(obtainedValue = loweffortAgencyCombined_SD,reportedValue = "230", valueType = 'sd') # match
```

When aggregation was done per the author's instructions, we could successfully reproduce all values.

### Inferential statistics

> Mean reproduction errors for each participant in each condition (see Fig. 3) were analysed using a 2 (“Presence of Agency”) × 2 (“Physical Effort”) ANOVA, revealing significant main effects of both of these factors: F(1, 34) = 54.54, p < 0.001, ηp2 = 0.62, and F(1, 34) = 14.43, p = 0.001, ηp2 = 0.3, respectively. 

> However, the critical interaction was non-significant, F(1, 34) = 2.12, p = 0.154, ηp2 = 0.06.

```{r}
## 2x2 anova to test main effects and interaction of agency & effort conditions 
expt1.tidy$agency=as.factor(expt1.tidy$agency)
expt1.tidy$effort=as.factor(expt1.tidy$effort)
expt1.tidy$PID=as.factor(expt1.tidy$PID)
expt1.tidy$rating=as.double(expt1.tidy$rating)

res1=ezANOVA(dv= .(rating), wid= .(PID), within= .(agency, effort), detailed=TRUE, data=data.frame(expt1.tidy)) # Note BL: had to convert to data frame.
unlisted <- unlist(res1)

aov_stats <- tibble(effect = c("agency", "effort", "agency:effort"),
  F = c(as.double(unlisted["ANOVA.F2"]), as.double(unlisted["ANOVA.F3"]), as.double(unlisted["ANOVA.F4"])), 
  p = c(as.double(unlisted["ANOVA.p2"]), as.double(unlisted["ANOVA.p3"]), as.double(unlisted["ANOVA.p4"])),
  SSn = c(as.double(unlisted["ANOVA.SSn2"]), as.double(unlisted["ANOVA.SSn3"]), as.double(unlisted["ANOVA.SSn4"])),
  SSd = c(as.double(unlisted["ANOVA.SSd2"]), as.double(unlisted["ANOVA.SSd3"]), as.double(unlisted["ANOVA.SSd4"]))) %>% 
  mutate(partial_eta_squared = SSn / (SSn + SSd)) %>% 
  select(-SSn, -SSd)

## Compare stats for main effects and interaction 

# agency
reportObject <- compareValues2(reportedValue = "1", obtainedValue = res1$ANOVA$DFn[2], valueType = 'df') #df1
reportObject <- compareValues2(reportedValue = "34", obtainedValue = res1$ANOVA$DFd[2], valueType = 'df') #df2

reportObject <- compareValues2(reportedValue = "54.54", obtainedValue = aov_stats$F[1], valueType = 'F') #agency main effect F statistic, minor error
reportObject <- compareValues2(reportedValue = "0.62", obtainedValue = aov_stats$partial_eta_squared[1], valueType = 'es') #agency main effect partial eta squared, match
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = aov_stats$p[1], valueType = 'p') #effort main effect partial eta squared, match

# agency p-value: MATCH (fits within reported interval)

# effort
reportObject <- compareValues2(reportedValue = "1", obtainedValue = res1$ANOVA$DFn[3], valueType = 'df') #df1
reportObject <- compareValues2(reportedValue = "34", obtainedValue = res1$ANOVA$DFd[3], valueType = 'df') #df2

reportObject <- compareValues2(reportedValue = "14.43", obtainedValue = aov_stats$F[2], valueType = 'F') #effort main effect F statistic, match
reportObject <- compareValues2(reportedValue = "0.3", obtainedValue = aov_stats$partial_eta_squared[2], valueType = 'es') #effort main effect partial eta squared, match
reportObject <- compareValues2(reportedValue = "0.001", obtainedValue = aov_stats$p[2], valueType = 'p') #effort main effect partial eta squared, match

# interaction
reportObject <- compareValues2(reportedValue = "1", obtainedValue = res1$ANOVA$DFn[4], valueType = 'df') #df1
reportObject <- compareValues2(reportedValue = "34", obtainedValue = res1$ANOVA$DFd[4], valueType = 'df') #df2

reportObject <- compareValues2(reportedValue = "2.12", obtainedValue = aov_stats$F[3], valueType = 'F') #effort*agency interaction F statistic, match
reportObject <- compareValues2(reportedValue = "0.06", obtainedValue = aov_stats$partial_eta_squared[3], valueType = 'es') #effort*agency interaction partial eta squared, match 
reportObject <- compareValues2(reportedValue = "0.154", obtainedValue = aov_stats$p[3], valueType = 'p') #effort*agency interaction p, match

```

Summary: For their main analysis to test their hypothesis, they ran a 2X2 anova testing the main effects and interaction of effort and agency conditions on the interval reproduction errors outcome variable. Replication of these anayleses yielded the same exact statistics reported in section 2.2 of their results section for experiment 1. There was one minor numerical errors in these findings. 

> "Although the interaction – critical to our research question – was non-significant, we nevertheless performed two planned contrasts to investigate the nature of effects across ‘‘Presence of Agency” and ‘‘Physical Effort”. In the ‘‘Agency” condition, reproduction errors were significantly larger under ‘‘Low” than ‘‘High” effort, t(34) = 3.46, p = 0.001, dz = 0.589, while this effect was not significant in the ‘‘No Agency” condition, t(34) = 1.59, p = 0.122, dz = 0.27."

```{r planned contrasts}
ttest1 <- t.test(expt1$A_lowE, expt1$A_highE, paired = T)
ttest2 <- t.test(expt1$NA_highE, expt1$NA_lowE , paired = T)

reportObject <- compareValues2(reportedValue = "34", obtainedValue = ttest1$parameter, valueType = 'df') # match
reportObject <- compareValues2(reportedValue = "-3.46", obtainedValue = ttest1$statistic, valueType = 't') # match
reportObject <- compareValues2(reportedValue = "0.001", obtainedValue = ttest1$p.value, valueType = 'p') # match

reportObject <- compareValues2(reportedValue = "34", obtainedValue = ttest2$parameter, valueType = 'df') # match
reportObject <- compareValues2(reportedValue = "1.59", obtainedValue = ttest2$statistic, valueType = 't') # minor error
reportObject <- compareValues2(reportedValue = "0.122", obtainedValue = ttest2$p.value, valueType = 'p') # match

# Calculate dz
dz1 <- mean(expt1$A_lowE - expt1$A_highE) / sd(expt1$A_lowE - expt1$A_highE)
dz2 <- mean(expt1$NA_lowE - expt1$NA_highE) / sd(expt1$NA_lowE - expt1$NA_highE)

reportObject <- compareValues2(reportedValue = "0.589", obtainedValue = abs(dz1), valueType = 'es') # minor error
reportObject <- compareValues2(reportedValue = "0.27", obtainedValue = abs(dz2), valueType = 'es') # match

```

## Step 5: Conclusion

This reproducibility check appears to be a success in terms of the main findings and statistics being reproduced. We encountered some difficulties reproducing the standard deviations at first. However, after discussing the issue with the authors, we discovered we were aggregating the data differently. When we followed the author's instructions, our reproducibility check was a complete success. 

```{r}
reportObject$Article_ID <- "DFDwT"
reportObject$affectsConclusion <- NA
reportObject$error_typo <- 0
reportObject$error_specification <- 0
reportObject$error_analysis <- 0
reportObject$error_data <- 0
reportObject$error_unidentified <- 0
reportObject$Author_Assistance <- T
reportObject$resolved_typo <- 0
reportObject$resolved_specification <- 1 
reportObject$resolved_analysis <- 0
reportObject$resolved_data <- 0
reportObject$correctionSuggested <- NA
reportObject$correctionPublished <- NA

# decide on final outcome
if(reportObject$Decision_Errors > 0 | reportObject$Major_Numerical_Errors > 0 | reportObject$Insufficient_Information_Errors > 0){
  reportObject$finalOutcome <- "Failure"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Failure despite author assistance"
  }
}else{
  reportObject$finalOutcome <- "Success"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Success with author assistance"
  }
}

# save the report object
filename <- paste0("reportObject_", reportObject$Article_ID,".csv")
write_csv(reportObject, filename)

```

## Report Object

```{r, echo = FALSE}
# display report object in chunks
kable(reportObject[2:10], align = 'l')
kable(reportObject[11:20], align = 'l')
kable(reportObject[21:25], align = 'l')
kable(reportObject[26:30], align = 'l')
kable(reportObject[31:35], align = 'l')
kable(reportObject[36:40], align = 'l')
kable(reportObject[41:45], align = 'l')
kable(reportObject[46:51], align = 'l')
kable(reportObject[52:57], align = 'l')
```

## Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
